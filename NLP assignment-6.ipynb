{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cc619d9e",
   "metadata": {},
   "source": [
    "1. What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea973f61",
   "metadata": {},
   "source": [
    "ans:\n",
    "Vanilla autoencoders are a type of neural network architecture that uses an encoder-decoder model to learn a compressed representation of data, known as an encoding, and then reconstructs the data from the encoding. The encoder part of the network takes in the input data and compresses it into a smaller representation, while the decoder part takes this smaller representation and reconstructs the original data as accurately as possible. The goal of an autoencoder is to learn a representation that captures the important features of the input data while minimizing the size of the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142304f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6dd19f5a",
   "metadata": {},
   "source": [
    "2. What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a162e78c",
   "metadata": {},
   "source": [
    "ans:\n",
    "Sparse autoencoders are a type of artificial neural network which uses a sparse data representation as a way of reducing the amount of data used in the network. The sparse representation is achieved by a regularization technique called sparsity, which forces the encoder to only use a small fraction of the available input neurons. This technique is used for feature learning and has been successfully applied to image recognition, text understanding, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff525f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "489d6d9a",
   "metadata": {},
   "source": [
    "3. What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b048d86",
   "metadata": {},
   "source": [
    "ans:\n",
    "Denoising autoencoders are a type of autoencoder neural network used for unsupervised learning. They are used to reduce noise from a signal by learning a representation of the input data. Denoising autoencoders work by adding noise to the input data, then training the network to reconstruct the original input from the noisy version. This helps the network to learn a robust representation of the data, which can be used for tasks such as classification and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7bdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4e407f40",
   "metadata": {},
   "source": [
    "4. What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e47e5d4",
   "metadata": {},
   "source": [
    "ans:\n",
    "Convolutional autoencoders are a type of autoencoder that use convolutional layers in the encoding and decoding of the data. They are used to learn useful features from the data and can be used for tasks such as image classification, image segmentation, and image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3316f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8c925629",
   "metadata": {},
   "source": [
    "5. What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3c377d0",
   "metadata": {},
   "source": [
    "ans:\n",
    "Stacked autoencoders are a type of deep learning neural network composed of multiple layers of autoencoders. These autoencoders are arranged in a stack, with each layer receiving input from the layer below it. The stacked autoencoders are used for feature extraction and representation learning. Each layer of the stack learns a representation of the data, which is then used as input to the next layer. This way, the deep learning model can learn a hierarchy of features and representations, allowing it to better capture the underlying structure of the data. Stacked autoencoders are a powerful tool for unsupervised learning, and have been used for a variety of tasks such as image recognition, dimensionality reduction, and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84c9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "837b7ac9",
   "metadata": {},
   "source": [
    "6. Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2e90dfd",
   "metadata": {},
   "source": [
    "ans:\n",
    "LSTM autoencoders are a type of recurrent neural network that is trained to generate text. The autoencoder is trained to take in a sequence of words and generate a corresponding output sequence. It does this by learning to map the input sequence to an internal representation and then reconstructing the output sequence from that representation. The autoencoder can then be used to generate new sentences from a given input by using the internal representation to generate new words or phrases. To generate sentences using an LSTM autoencoder, the model must first be trained on a large corpus of text, such as a book or a collection of news articles. Once trained, the autoencoder can be used to generate new sentences by providing it with a seed sentence or phrase. The autoencoder will then generate a sequence of words based on the seed phrase. This process can be repeated until a satisfactory sentence or paragraph is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30a4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "47840ba9",
   "metadata": {},
   "source": [
    "7. Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "282b2cc4",
   "metadata": {},
   "source": [
    "ans:\n",
    "Extractive summarization is the process of automatically creating a summary by identifying and extracting relevant phrases and sentences from a source document. It involves selecting important pieces of text from the source document and concatenating them to form a summary. It is a type of text summarization technique that focuses on finding the most important sentences or phrases in a document and extracting them to create a summary. This type of summarization is useful for extracting key facts and ideas from a large body of text, such as a book or article, and condensing them into a shorter summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b1c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "50c896bf",
   "metadata": {},
   "source": [
    "8. Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af949866",
   "metadata": {},
   "source": [
    "ans:\n",
    "Abstractive summarization is a type of summarization technique that generates new phrases and sentences to accurately capture the meaning and essence of the original text. Unlike extractive summarization, abstractive techniques rephrase the original text and may include the author's own words and interpretations. Abstractive summarization methods use natural language processing and deep learning algorithms to generate summaries. They are often used in text summarization, question answering, document summarization, and machine translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec23e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "89763af5",
   "metadata": {},
   "source": [
    "9. Explain Beam search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a6d2899",
   "metadata": {},
   "source": [
    "ans:\n",
    "Beam search is an algorithm used in artificial intelligence to find the most probable sequence of words in natural language processing, machine translation, and other applications. It is an extension of the breadth-first search algorithm. The main difference between beam search and breadth-first search is that beam search uses a fixed-width beam instead of exploring all nodes at a given depth. This helps to reduce the amount of time required to find a solution, while still allowing the algorithm to explore multiple paths. Beam search also uses heuristics, such as the number of words in the sentence, to prune the search space and make the algorithm more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961d1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "de9cce77",
   "metadata": {},
   "source": [
    "10. Explain Length normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c1374d4",
   "metadata": {},
   "source": [
    "ans:\n",
    "Length normalization is a technique used to normalize the lengths of text strings in a corpus in order to make them comparable. The goal is to make sure that the same text strings are being compared, regardless of their length. This is often done by dividing the length of a text string by the maximum length of a text string in the corpus. This helps to ensure that all text strings are being compared on an equal basis, regardless of their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee69b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0ee9370c",
   "metadata": {},
   "source": [
    "11. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35ba77ef",
   "metadata": {},
   "source": [
    "ans:\n",
    "Coverage normalization is a method used to normalize the coverage of sequencing reads across multiple samples. It is used to account for the uneven coverage of sequencing reads due to varying library sizes or sequencing depth across samples. By normalizing coverage, researchers can compare samples with different sequencing depths on a more equal footing. Coverage normalization is often accomplished by calculating a “scaling factor” for each sample, which is used to multiply the sequencing depth of that sample to match the median sequencing depth of the entire dataset. This then allows the comparison of sequencing data from different samples on a normalized basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b814040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "94e470db",
   "metadata": {},
   "source": [
    "12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4641b4fb",
   "metadata": {},
   "source": [
    "ans:\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric evaluation used to measure the quality of a summarization task. It is used to compare the automatically produced summary with a set of reference summaries that were created by humans. The ROUGE evaluation compares the summaries of two different systems and computes a score based on the number of overlapping n-grams between them. It also takes into account the length of the summary by comparing it to the average length of the reference summaries. ROUGE is a popular metric for measuring summarization quality and is used in many text summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245004fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
